---
title: 'Text Mining and Social Media Mining Project'
subtitle: 'Clustering and Topic Modelling'
author: 
- Szymon Socha
- Micha≈Ç Kunstler
date: '2023-01-06'
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      smooth_scroll: true
    theme: paper
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data description

This project used a dataset containing Australian news headlines over a nineteen-year period (2003-2021). The news source is the reputable Australian news source ABC (Australian Broadcasting Corporation), which was downloaded from Kaggle.

The dataset consists of more than 1.2 million **headlines** along with the **dates** they were published. 

Since the dataset is large, we decide to narrow it down to two selected years - 2004 and 2020. This will significantly speed up the time required for calculations and allow us to compare the results of the analysis for two different periods.

```{r libraries_load_data, message=FALSE, warning=FALSE}
# Clustering
library(tm)
library(wordcloud)
library(factoextra)
library(NbClust)
library(cluster)
library(dplyr)
library(lubridate)
library(stringr)
library(textmineR)

# Topic Modelling
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modelling 
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming

# Data source:
# https://www.kaggle.com/datasets/therohk/million-headlines

raw_data <- read.csv('data/abcnews-date-text.csv') %>%
  mutate(publish_date = ymd(publish_date))

raw_data <- raw_data %>% 
  tibble::rownames_to_column(var = "id")

data_2004 <- raw_data %>%
  filter(between(publish_date, as.Date("2004-01-01"), as.Date("2004-12-31")))

data_2020 <- raw_data %>%
  filter(between(publish_date, as.Date("2020-01-01"), as.Date("2020-12-31"))) %>% 
  # consider covid as the same thing as coronavirus
  mutate(across('headline_text', str_replace, 'covid', 'coronavirus'))
```

# Year of 2004

## Clustering

In this chapter, we perform k-means clustering of the text for 2004. We select the optimal number of clusters using the Elbow method and the Silhouette method. Then we check if the number of clusters so selected is correct. At the end, we visualize the clusters using wordclouds.

### Data preparation

We perform text cleaning remove stopwords. Since the analysis is only about Australia we decide to remove words that are related to the geography of Australia (australia, australian, nsw, queensland, victoria).

We also reduce the sparsity and calculate the distance matrix using the euclidian method.

```{r load_cleaning_2004}
docs_2004 <- VCorpus(VectorSource(data_2004$headline_text))

# Preliminary cleaning, Cleaning text and Stopword removal ----
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))

docs_2004 <- tm_map(docs_2004, removePunctuation)
docs_2004 <- tm_map(docs_2004, removeNumbers)
docs_2004 <- tm_map(docs_2004, content_transformer(tolower))
docs_2004 <- tm_map(docs_2004, removeWords, stopwords("english"))
docs_2004 <- tm_map(docs_2004, removeWords, c("australia", "australian", "nsw", "queensland", "victoria")) # since the source data is from Australia, remove this word from the dataset
docs_2004 <- tm_map(docs_2004, stripWhitespace)

# Create document-term matrix
tdm_2004 <- TermDocumentMatrix(docs_2004)

# Reduce sparcity
tdms_2004 <- removeSparseTerms(tdm_2004, sparse = 0.99)

# Calculate distance matrix
best_n_matrix_2004 <- as.matrix(dist(tdms_2004, method="euclidian"))
```

### Find optimal number of clusters

It is very important to find the optimal number of clusters. We propose two approaches. One based solely on statistical methods. The other based on observations resulting from abitrary selection of number of clusters. Then we compare which method gives better results.

#### Appropriate number of clusters based on statistical method

To find the optimal number of clusters, we use the elbow and silhouette methods. Later we will also see if the number of clusters selected in this way is correct.

##### Elbow method 

The elbow method involves plotting a curve showing the relationship between the number of clusters and prediction quality. It is based on an arbitrary decision to choose the optimal number of clusters resulting from the diminishing marginal benefit of adding another cluster.

```{r elbow_2004}
fviz_nbclust(best_n_matrix_2004, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(subtitle = "Elbow method")
```

Elbow method does not clearly indicate what number of clusters is correct. However, after the 2nd cluster, a slight decrease in slope is evident.

##### Silhouette method

The silhouette Method can also be used to interpret and validate consistency inside data clusters, as well as to determine the ideal number of clusters. Each point's silhouette coefficient, which quantifies how much a point resembles its own cluster in relation to other clusters, is computed using the silhouette method.

Compared to the elbow method, it is possible to say clearly what number of clusters is the most correct.

```{r silhouette_2004}
fviz_nbclust(best_n_matrix_2004, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Like the elbow method, the silhouette method shows that the correct number of clusters is 2.

##### Visualize clusters

Now let's check how the text clustering will look like for two clusters.

###### Clustplot

Plot a 2D graph and let's see what our clusters look like.

```{r clustplot_2_2004}
kfit_2004 <- kmeans(dist(tdms_2004, method="euclidian"), 2)

clusplot(as.matrix(dist(tdms_2004, method="euclidian")), kfit_2004$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```
 
It can be observed that the clusters are quite well grouped. Of which one cluster is more numerous than the other.

###### Hierarchical clustering

Let's see what it looks like on the tree diagram some more.

```{r hier_2_2004}
d_2004 <- dist(tdms_2004, method="euclidian")

hc_2004 <- hclust(d_2004, "ward.D")

# Kmeans
clustering_2004 <- kfit_2004$cluster

plot(hc_2004, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

We observe that one of the branches is much more extensive than the others. The other branches are relatively uncomplicated.

```{r summ_2_2004, echo=FALSE, include=FALSE}
p_words_2004 <- colSums(t(as.matrix(tdms_2004))) / sum(t(as.matrix(tdms_2004)))

cluster_words_2004 <- lapply(unique(clustering_2004), function(x){
  rows_2004 <- tdms_2004[ clustering_2004 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2004 <- rows_2004[ , colSums(t(as.matrix(rows_2004))) > 0 ]
  
  colSums(t(as.matrix(rows_2004))) / sum(t(as.matrix(rows_2004))) - p_words_2004[ colnames(t(as.matrix(rows_2004))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2004 <- data.frame(cluster = unique(clustering_2004),
                                   size = as.numeric(table(clustering_2004)),
                                   top_words = sapply(cluster_words_2004, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2004
```

###### Word Cloud for Cluster No. 1

```{r wordcloud_1_2_2004}
wordcloud(words = names(cluster_words_2004[[ 1 ]]), 
          freq = cluster_words_2004[[ 1 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Wordcloud for the first cluster indicates that it illustrates the new theme of the 2004 Iraq war and the government's related plan.

###### Word Cloud for Cluster No. 2

```{r wordcloud_2_2_2004}
wordcloud(words = names(cluster_words_2004[[ 2 ]]), 
          freq = cluster_words_2004[[ 2 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Wordcloud for the second cluster is related to the 2004 election that took place in Australia and the newly formed government.

#### Appropriate number of clusters based on empirics

The number of 2 clusters selected by elbow and silhouette methods seems quite small for a whole year of different events. We decide to arbitrarily choose 4 clusters and look at what the results look like for more clusters.

##### Visualize clusters

###### Clustplot

Again, let's look at the 2D plot.

```{r clust_4_2004}
kfit_2004 <- kmeans(dist(tdms_2004, method="euclidian"), 4)

clusplot(as.matrix(dist(tdms_2004, method="euclidian")), kfit_2004$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

This time we get two small clusters, into one of which the police fall, into the other the subject of the new government. This time the clusters are more compact and seem to make more sense.

###### Hierarchical clustering

```{r hier_4_2004}
d_2004 <- dist(tdms_2004, method="euclidian")

hc_2004 <- hclust(d_2004, "ward.D")

# Kmeans
clustering_2004 <- kfit_2004$cluster

plot(hc_2004, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

As expected, the hierarchical tree looks exactly the same.

```{r summ_4_2004, echo=FALSE, include=FALSE}
p_words_2004 <- colSums(t(as.matrix(tdms_2004))) / sum(t(as.matrix(tdms_2004)))

cluster_words_2004 <- lapply(unique(clustering_2004), function(x){
  rows_2004 <- tdms_2004[ clustering_2004 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2004 <- rows_2004[ , colSums(t(as.matrix(rows_2004))) > 0 ]
  
  colSums(t(as.matrix(rows_2004))) / sum(t(as.matrix(rows_2004))) - p_words_2004[ colnames(t(as.matrix(rows_2004))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2004 <- data.frame(cluster = unique(clustering_2004),
                                   size = as.numeric(table(clustering_2004)),
                                   top_words = sapply(cluster_words_2004, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2004
```

###### Word Cloud for Cluster No. 1

```{r wordcloud_1_4_2004}
wordcloud(words = names(cluster_words_2004[[ 1 ]]), 
          freq = cluster_words_2004[[ 1 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The first cluster can be explained as headlining related to the court's statements related to the fires. 

###### Word Cloud for Cluster No. 2

```{r wordcloud_2_4_2004}
wordcloud(words = names(cluster_words_2004[[ 2 ]]), 
          freq = cluster_words_2004[[ 2 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The second cluster is related to the council's plans for war in Iraq.

###### Word Cloud for Cluster No. 3

```{r wordcloud_3_4_2004}
wordcloud(words = names(cluster_words_2004[[ 3 ]]), 
          freq = cluster_words_2004[[ 3 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The third cluster is related to the 2004 elections and the new government.

###### Word Cloud for Cluster No. 4

```{r wordcloud_4_4_2004}
wordcloud(words = names(cluster_words_2004[[ 4 ]]), 
          freq = cluster_words_2004[[ 4 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The fourth cluster is related to the police theme. We think this may be related to the fact that the number of topics that link to police intervention is simply predominant.

### Conclusions

In this chapter, we did text clustering on a narrowed date range of 2004. We checked whether the number of clusters indicated by statistical methods (elbow method and silhouette method) is the best possible method to indicate the correct number of clusters.  To do this, we compared clustering by statistical methods with the number of clusters chosen arbitrarily.

The elbow and silhouette methods indicated that the best number of clusters is 2. We arbitrarily indicated 4 clusters. In our opinion, clustering up to **4 clusters** is better.

We made an attempt to explain these clusters. These clusters describe the following topics:

* **judiciary**

* **Iraq war politics**

* **elections**

* **general topics related to police intervention**

## Topic Modelling

Another topic we are addressing is topic modelling. This is the second unsupervised method besides text clustering for discovering text structures. Unlike clustering, topic modelling allows more than one topic to be assigned to a given text (clustering allows a given text to be assigned to only one cluster).

For topic modeling, we use Latent Dirichlet Allocation (LDA), which is the most prominent topic modeling method currently in use. 

LDA uses *Dirichlet distribution*, which is called *Dirichlet prior*. This is used both to assign topics to documents and to find words for topics.

### Data preparation 

```{r topic_modeling_preli_2004}
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
                                   plot = T, # return a plot? TRUE by defult
                                   number_of_topics = 8) # number of topics (4 by default)
{    
  # create a corpus (type of object expected by tm) and document term matrix
  Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
  DTM <- DocumentTermMatrix(Corpus) # get the count of words/document
  
  # remove any empty rows in our document term matrix (if there are any 
  # we'll get an error when we try to run our LDA)
  unique_indexes <- unique(DTM$i) # get the index of each unique value
  DTM <- DTM[unique_indexes,] # get a subset of only those indexes
  
  # preform LDA & get the words/topic in a tidy text format
  lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
  topics <- tidy(lda, matrix = "beta")
  
  # get the top ten terms for each topic
  top_terms <- topics  %>% # take the topics data frame and..
    group_by(topic) %>% # treat each topic as a different group
    top_n(10, beta) %>% # get the top 10 most informative words
    ungroup() %>% # ungroup
    arrange(topic, -beta) # arrange words in descending informativeness
  
  # if the user asks for a plot (TRUE by default)
  if(plot == T){
    # plot the top ten terms for each topic in order
    top_terms %>% # take the top terms
      mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
      ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
      geom_col(show.legend = FALSE) + # as a bar plot
      facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
      labs(x = NULL, y = "Beta") + # no x label, change y label 
      coord_flip() # turn bars sideways
  }else{ 
    # if the user does not request a plot
    # return a list of sorted terms instead
    return(top_terms)
  }
}

# create a document term matrix to clean
Corpus_2004 <- Corpus(VectorSource(data_2004$headline_text)) 
DTM_2004 <- DocumentTermMatrix(Corpus_2004)

# convert the document term matrix to a tidytext corpus
DTM_tidy_2004 <- tidy(DTM_2004)

# I'm going to add my own custom stop words that I don't think will be
# very informative
custom_stop_words_2004 <- tibble(word = c("australia", "australian", 
                                          "queensland", "nsw", "victoria"))

# remove stopwords
DTM_tidy_cleaned_2004 <- DTM_tidy_2004 %>% # take our tidy dtm and...
  anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
  anti_join(custom_stop_words_2004, by = c("term" = "word")) # remove my custom stopwords

# reconstruct cleaned documents (so that each word shows up the correct number of times)
# stem the words (e.g. convert each word to its stem, where applicable)
cleaned_documents_2004 <- DTM_tidy_cleaned_2004 %>%
  group_by(document) %>% 
  mutate(terms = toString(rep(wordStem(term), count))) %>%
  select(document, terms) %>%
  unique()
```

### Plot

```{r top_terms_2004}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2004$terms, number_of_topics = 4)
```

# Year of 2020

## Clustering

*The folder "text_files" contains text files written on various topics by two persons. Interests of person 1 and person 2 are very distinct. Note that the file title doesn't necessarily reflect its content. Use a chosen method of clustering in order to find into which categories/concepts/topics belong these files. Can you say anything relevant about types of these texts or, maybe, about their authors?*

### Data cleaning

```{r load_cleaning_2020}
docs_2020 <- VCorpus(VectorSource(data_2020$headline_text))

# Preliminary cleaning, Cleaning text and Stopword removal ----
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))

docs_2020 <- tm_map(docs_2020, removePunctuation)
docs_2020 <- tm_map(docs_2020, removeNumbers)
docs_2020 <- tm_map(docs_2020, content_transformer(tolower))
docs_2020 <- tm_map(docs_2020, removeWords, stopwords("english"))
docs_2020 <- tm_map(docs_2020, removeWords, c("australia", "australian", "nsw", "queensland", "victoria")) # since the source data is from Australia, remove this word from the dataset
docs_2020 <- tm_map(docs_2020, stripWhitespace)

# Create document-term matrix
tdm_2020 <- TermDocumentMatrix(docs_2020)
tdm_2020

# Reduce sparcity
tdms_2020 <- removeSparseTerms(tdm_2020, sparse = 0.99)
tdms_2020

# Calculate distance matrix
best_n_matrix_2020 <- as.matrix(dist(tdms_2020, method="euclidian"))
```

### Find optimal number of clusters

I use the Elbow method and the Silhouette method to select the optimal number of clusters.

#### Elbow method

```{r elbow_2020}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(subtitle = "Elbow method")
```

#### Silhouette method

```{r silhouette_2020}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

I choose ? clusters.

### Visualize clusters

#### Clustplot

```{r clust_2_2020}
kfit_2020 <- kmeans(dist(tdms_2020, method="euclidian"), 2)

clusplot(as.matrix(dist(tdms_2020, method="euclidian")), kfit_2020$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

#### Hierarchical clustering

```{r hier_2_2020}
d_2020 <- dist(tdms_2020, method="euclidian")

hc_2020 <- hclust(d_2020, "ward.D")


# Hierarchical
#clustering <- cutree(hc, 4)

# Kmeans
clustering_2020 <- kfit_2020$cluster


plot(hc_2020, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

It can be observed that the two groups are significantly different. The first group is much more complex than the second.

```{r summ_2020, echo=FALSE, include=FALSE}
p_words_2020 <- colSums(t(as.matrix(tdms_2020))) / sum(t(as.matrix(tdms_2020)))

cluster_words_2020 <- lapply(unique(clustering_2020), function(x){
  rows_2020 <- tdms_2020[ clustering_2020 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2020 <- rows_2020[ , colSums(t(as.matrix(rows_2020))) > 0 ]
  
  colSums(t(as.matrix(rows_2020))) / sum(t(as.matrix(rows_2020))) - p_words_2020[ colnames(t(as.matrix(rows_2020))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2020 <- data.frame(cluster = unique(clustering_2020),
                                   size = as.numeric(table(clustering_2020)),
                                   top_words = sapply(cluster_words_2020, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2020
```

#### Word Cloud for Cluster No. 1

```{r wordcloud_1_2_2020}
wordcloud(words = names(cluster_words_2020[[ 1 ]]), 
          freq = cluster_words_2020[[ 1 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Analysing the word cloud above, I find that they come from the **clothing industry**.

#### Word Cloud for Cluster No. 2

```{r wordcloud_2_2_2020}
wordcloud(words = names(cluster_words_2020[[ 2 ]]), 
          freq = cluster_words_2020[[ 2 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Analysing the word cloud above, I find that they come from the **gaming industry**.

### Additional analysis

```{r load_cleaning_no_covid}
docs_2020 <- tm_map(docs_2020, removeWords, c("coronavirus")) # remove 'coronavirus'

# Create document-term matrix
tdm_2020 <- TermDocumentMatrix(docs_2020)
tdm_2020

# Reduce sparcity
tdms_2020 <- removeSparseTerms(tdm_2020, sparse = 0.99)
tdms_2020

# Calculate distance matrix
best_n_matrix_2020 <- as.matrix(dist(tdms_2020, method="euclidian"))
```

#### Find optimal number of clusters

I use the Elbow method and the Silhouette method to select the optimal number of clusters.

##### Elbow method

```{r elbow_2020_no_covid}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(subtitle = "Elbow method")
```

##### Silhouette method

```{r silhouette_2020_no_covid}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

I choose ? clusters.

#### Visualize clusters

##### Clustplot

```{r clust_2020_no_covid}
kfit_2020 <- kmeans(dist(tdms_2020, method="euclidian"), 2)

clusplot(as.matrix(dist(tdms_2020, method="euclidian")), kfit_2020$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

##### Hierarchical clustering

```{r hier_2020_no_covid}
d_2020 <- dist(tdms_2020, method="euclidian")

hc_2020 <- hclust(d_2020, "ward.D")

# Kmeans
clustering_2020 <- kfit_2020$cluster

plot(hc_2020, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

It can be observed that the two groups are significantly different. The first group is much more complex than the second.

```{r summ_2020_no_covid, echo=FALSE, include=FALSE}
p_words_2020 <- colSums(t(as.matrix(tdms_2020))) / sum(t(as.matrix(tdms_2020)))

cluster_words_2020 <- lapply(unique(clustering_2020), function(x){
  rows_2020 <- tdms_2020[ clustering_2020 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2020 <- rows_2020[ , colSums(t(as.matrix(rows_2020))) > 0 ]
  
  colSums(t(as.matrix(rows_2020))) / sum(t(as.matrix(rows_2020))) - p_words_2020[ colnames(t(as.matrix(rows_2020))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2020 <- data.frame(cluster = unique(clustering_2020),
                                   size = as.numeric(table(clustering_2020)),
                                   top_words = sapply(cluster_words_2020, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2020
```

##### Word Cloud for Cluster No. 1

```{r wordcloud_1_2_2020_no_covid}
wordcloud(words = names(cluster_words_2020[[ 1 ]]), 
          freq = cluster_words_2020[[ 1 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Analysing the word cloud above, I find that they come from the **clothing industry**.

##### Word Cloud for Cluster No. 2

```{r wordcloud_2_2_2020_no_covid}
wordcloud(words = names(cluster_words_2020[[ 2 ]]), 
          freq = cluster_words_2020[[ 2 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Analysing the word cloud above, I find that they come from the **gaming industry**.

### Conclusions

## Topic Modelling

```{r}
# create a document term matrix to clean
Corpus_2020 <- Corpus(VectorSource(data_2020$headline_text)) 
DTM_2020 <- DocumentTermMatrix(Corpus_2020)
```

```{r}
# convert the document term matrix to a tidytext corpus
DTM_tidy_2020 <- tidy(DTM_2020)
```

```{r}
# I'm going to add my own custom stop words that I don't think will be
# very informative
custom_stop_words_2020 <- tibble(word = c("australia", "australian", 
                                          "queensland", "nsw", "victoria",
                                          "coronavirus"))
```

```{r}
# remove stopwords
DTM_tidy_cleaned_2020 <- DTM_tidy_2020 %>% # take our tidy dtm and...
  anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
  anti_join(custom_stop_words_2020, by = c("term" = "word")) # remove my custom stopwords
```

```{r}
# reconstruct cleaned documents (so that each word shows up the correct number of times)
# stem the words (e.g. convert each word to its stem, where applicable)
cleaned_documents_2020 <- DTM_tidy_cleaned_2020 %>%
  group_by(document) %>% 
  mutate(terms = toString(rep(wordStem(term), count))) %>%
  select(document, terms) %>%
  unique()
```

```{r}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2020$terms, number_of_topics = 2)
```

# Conclusions


