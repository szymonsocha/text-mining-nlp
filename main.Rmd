---
title: 'Text Mining and Social Media Mining Project'
subtitle: 'Clustering and Topic Modeling'
author: 
- Szymon Socha
- Micha≈Ç Kunstler
date: '2023-01-06'
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      smooth_scroll: true
    theme: paper
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data description

This project used a dataset containing Australian news headlines over a nineteen-year period (2003-2021). The news source is the reputable Australian news source ABC (Australian Broadcasting Corporation), which was downloaded from [Kaggle](https://www.kaggle.com/datasets/therohk/million-headlines).

The dataset consists of more than 1.2 million **headlines** along with the **dates** they were published. 

Since the dataset is large, we decide to narrow it down to two selected years - 2004 and 2020. This will significantly speed up the time required for calculations and allow us to compare the results of the analysis for two different periods.

```{r libraries_load_data, message=FALSE, warning=FALSE}
# Clustering
library(tm)
library(wordcloud)
library(factoextra)
library(NbClust)
library(cluster)
library(dplyr)
library(lubridate)
library(stringr)
library(textmineR)

# Topic Modeling
library(tidyverse) # general utility & workflow functions
library(tidytext) # tidy implimentation of NLP methods
library(topicmodels) # for LDA topic modeling 
library(tm) # general text mining functions, making document term matrixes
library(SnowballC) # for stemming

# Data source:
# https://www.kaggle.com/datasets/therohk/million-headlines

raw_data <- read.csv('data/abcnews-date-text.csv') %>%
  mutate(publish_date = ymd(publish_date))

raw_data <- raw_data %>% 
  tibble::rownames_to_column(var = "id")

data_2004 <- raw_data %>%
  filter(between(publish_date, as.Date("2004-01-01"), as.Date("2004-12-31")))

data_2020 <- raw_data %>%
  filter(between(publish_date, as.Date("2020-01-01"), as.Date("2020-12-31"))) %>% 
  # consider covid as the same thing as coronavirus
  mutate(across('headline_text', str_replace, 'covid', 'coronavirus'))
```

# Year of 2004

## Clustering

In this chapter, we perform k-means clustering of the text for 2004. We select the optimal number of clusters using the Elbow method and the Silhouette method. Then we check if the number of clusters so selected is correct. At the end, we visualize the clusters using wordclouds.

### Data preparation

We perform text cleaning remove stopwords. Since the analysis is only about Australia we decide to remove words that are related to the geography of Australia (australia, australian, nsw, queensland, victoria).

We also reduce the sparsity and calculate the distance matrix using the euclidian method.

```{r load_cleaning_2004}
docs_2004 <- VCorpus(VectorSource(data_2004$headline_text))

# Preliminary cleaning, Cleaning text and Stopword removal ----
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))

docs_2004 <- tm_map(docs_2004, removePunctuation)
docs_2004 <- tm_map(docs_2004, removeNumbers)
docs_2004 <- tm_map(docs_2004, content_transformer(tolower))
docs_2004 <- tm_map(docs_2004, removeWords, stopwords("english"))
docs_2004 <- tm_map(docs_2004, removeWords, c("australia", "australian", "nsw", "queensland", "victoria")) # since the source data is from Australia, remove this word from the dataset
docs_2004 <- tm_map(docs_2004, stripWhitespace)

# Create document-term matrix
tdm_2004 <- TermDocumentMatrix(docs_2004)

# Reduce sparcity
tdms_2004 <- removeSparseTerms(tdm_2004, sparse = 0.99)

# Calculate distance matrix
best_n_matrix_2004 <- as.matrix(dist(tdms_2004, method="euclidian"))
```

### Find optimal number of clusters

It is very important to find the optimal number of clusters. We propose two approaches. One based solely on statistical methods. The other based on observations resulting from abitrary selection of number of clusters. Then we compare which method gives better results.

#### Appropriate number of clusters based on statistical method

To find the optimal number of clusters, we use the elbow and silhouette methods. Later we will also see if the number of clusters selected in this way is correct.

##### Elbow method 

The elbow method involves plotting a curve showing the relationship between the number of clusters and prediction quality. It is based on an arbitrary decision to choose the optimal number of clusters resulting from the diminishing marginal benefit of adding another cluster.

```{r elbow_2004}
fviz_nbclust(best_n_matrix_2004, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(subtitle = "Elbow method")
```

Elbow method does not clearly indicate what number of clusters is correct. However, after the 2nd cluster, a slight decrease in slope is evident.

##### Silhouette method

The silhouette Method can also be used to interpret and validate consistency inside data clusters, as well as to determine the ideal number of clusters. Each point's silhouette coefficient, which quantifies how much a point resembles its own cluster in relation to other clusters, is computed using the silhouette method.

Compared to the elbow method, it is possible to say clearly what number of clusters is the most correct.

```{r silhouette_2004}
fviz_nbclust(best_n_matrix_2004, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Like the elbow method, the silhouette method shows that the correct number of clusters is 2.

##### Visualize clusters

Now let's check how the text clustering will look like for two clusters.

###### Clustplot

Plot a 2D graph and let's see what our clusters look like.

```{r clustplot_2_2004}
kfit_2004 <- kmeans(dist(tdms_2004, method="euclidian"), 2)

clusplot(as.matrix(dist(tdms_2004, method="euclidian")), kfit_2004$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```
 
It can be observed that the clusters are quite well grouped. Of which one cluster is more numerous than the other.

###### Hierarchical clustering

Let's see what it looks like on the tree diagram some more.

```{r hier_2_2004}
d_2004 <- dist(tdms_2004, method="euclidian")

hc_2004 <- hclust(d_2004, "ward.D")

# Kmeans
clustering_2004 <- kfit_2004$cluster

plot(hc_2004, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

We observe that one of the branches is much more extensive than the others. The other branches are relatively uncomplicated.

```{r summ_2_2004, echo=FALSE, include=FALSE}
p_words_2004 <- colSums(t(as.matrix(tdms_2004))) / sum(t(as.matrix(tdms_2004)))

cluster_words_2004 <- lapply(unique(clustering_2004), function(x){
  rows_2004 <- tdms_2004[ clustering_2004 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2004 <- rows_2004[ , colSums(t(as.matrix(rows_2004))) > 0 ]
  
  colSums(t(as.matrix(rows_2004))) / sum(t(as.matrix(rows_2004))) - p_words_2004[ colnames(t(as.matrix(rows_2004))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2004 <- data.frame(cluster = unique(clustering_2004),
                                   size = as.numeric(table(clustering_2004)),
                                   top_words = sapply(cluster_words_2004, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2004
```

###### Word Cloud for Cluster No. 1

```{r wordcloud_1_2_2004}
wordcloud(words = names(cluster_words_2004[[ 1 ]]), 
          freq = cluster_words_2004[[ 1 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Wordcloud for the first cluster indicates that it illustrates the new topic of the 2004 **Iraq war and the government's related plan**.

###### Word Cloud for Cluster No. 2

```{r wordcloud_2_2_2004}
wordcloud(words = names(cluster_words_2004[[ 2 ]]), 
          freq = cluster_words_2004[[ 2 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

Wordcloud for the second cluster is related to the 2004 **election** that took place in Australia and the **newly formed government**.

#### Appropriate number of clusters based on empirics

The number of 2 clusters selected by elbow and silhouette methods seems quite small for a whole year of different events. We decide to arbitrarily choose 4 clusters and look at what the results look like for more clusters.

##### Visualize clusters

###### Clustplot

Again, let's look at the 2D plot.

```{r clust_4_2004}
kfit_2004 <- kmeans(dist(tdms_2004, method="euclidian"), 4)

clusplot(as.matrix(dist(tdms_2004, method="euclidian")), kfit_2004$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

This time we get two small clusters, into one of which the police fall, into the other the subject of the new government. This time the clusters are more compact and seem to make more sense.

###### Hierarchical clustering

```{r hier_4_2004}
d_2004 <- dist(tdms_2004, method="euclidian")

hc_2004 <- hclust(d_2004, "ward.D")

# Kmeans
clustering_2004 <- kfit_2004$cluster

plot(hc_2004, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

As expected, the hierarchical tree looks exactly the same.

```{r summ_4_2004, echo=FALSE, include=FALSE}
p_words_2004 <- colSums(t(as.matrix(tdms_2004))) / sum(t(as.matrix(tdms_2004)))

cluster_words_2004 <- lapply(unique(clustering_2004), function(x){
  rows_2004 <- tdms_2004[ clustering_2004 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2004 <- rows_2004[ , colSums(t(as.matrix(rows_2004))) > 0 ]
  
  colSums(t(as.matrix(rows_2004))) / sum(t(as.matrix(rows_2004))) - p_words_2004[ colnames(t(as.matrix(rows_2004))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2004 <- data.frame(cluster = unique(clustering_2004),
                                   size = as.numeric(table(clustering_2004)),
                                   top_words = sapply(cluster_words_2004, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2004
```

###### Word Cloud for Cluster No. 1

```{r wordcloud_1_4_2004}
wordcloud(words = names(cluster_words_2004[[ 1 ]]), 
          freq = cluster_words_2004[[ 1 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The first cluster can be explained as headlining related to the **court's statements related to the fires**. 

###### Word Cloud for Cluster No. 2

```{r wordcloud_2_4_2004}
wordcloud(words = names(cluster_words_2004[[ 2 ]]), 
          freq = cluster_words_2004[[ 2 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The second cluster is related to the **council's plans for war in Iraq**.

###### Word Cloud for Cluster No. 3

```{r wordcloud_3_4_2004}
wordcloud(words = names(cluster_words_2004[[ 3 ]]), 
          freq = cluster_words_2004[[ 3 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The third cluster is related to the **2004 elections and the new government**.

###### Word Cloud for Cluster No. 4

```{r wordcloud_4_4_2004}
wordcloud(words = names(cluster_words_2004[[ 4 ]]), 
          freq = cluster_words_2004[[ 4 ]], max.words=4, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The fourth cluster is related to the **police** topic. We think this may be related to the fact that the number of topics that link to police intervention is simply predominant.

### Conclusions

In this chapter, we did text clustering on a narrowed date range of 2004. We checked whether the number of clusters indicated by statistical methods (elbow method and silhouette method) is the best possible method to indicate the correct number of clusters.  To do this, we compared clustering by statistical methods with the number of clusters chosen arbitrarily.

The elbow and silhouette methods indicated that the best number of clusters is 2. We arbitrarily indicated 4 clusters. In our opinion, clustering up to **4 clusters** is better.

We made an attempt to explain these clusters. These clusters describe the following topics:

* **judiciary**

* **Iraq war politics**

* **elections**

* **general topics related to police intervention**

## Topic Modeling

Another topic we are addressing is topic modeling. This is the second unsupervised method besides text clustering for discovering text structures. Unlike clustering, topic modeling allows more than one topic to be assigned to a given text (clustering allows a given text to be assigned to only one cluster).

For topic modeling, we use Latent Dirichlet Allocation (LDA), which is the most prominent topic modeling method currently in use. 

LDA uses *Dirichlet distribution*, which is called *Dirichlet prior*. This is used both to assign topics to documents and to find words for topics.

### Data preparation 

In order to prepare data for topic modeling, we create Corpus and Document Term Matrix. As with clustering, we remove words related to Australia's geography (such as *australia*, *australian*, *queensland*, *nsw*, *victoria*).
We also remove stopwords and do stemming.

Below are the cleaned texts.

```{r topic_modeling_preli_2004}
top_terms_by_topic_LDA <- function(input_text, # should be a columm from a dataframe
                                   plot = T, # return a plot? TRUE by defult
                                   number_of_topics = 8) # number of topics (4 by default)
{    
  # create a corpus (type of object expected by tm) and document term matrix
  Corpus <- Corpus(VectorSource(input_text)) # make a corpus object
  DTM <- DocumentTermMatrix(Corpus) # get the count of words/document
  
  # remove any empty rows in our document term matrix (if there are any 
  # we'll get an error when we try to run our LDA)
  unique_indexes <- unique(DTM$i) # get the index of each unique value
  DTM <- DTM[unique_indexes,] # get a subset of only those indexes
  
  # preform LDA & get the words/topic in a tidy text format
  lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
  topics <- tidy(lda, matrix = "beta")
  
  # get the top ten terms for each topic
  top_terms <- topics  %>% # take the topics data frame and..
    group_by(topic) %>% # treat each topic as a different group
    top_n(3, beta) %>% # get the top 4 most informative words
    ungroup() %>% # ungroup
    arrange(topic, -beta) # arrange words in descending informativeness
  
  # if the user asks for a plot (TRUE by default)
  if(plot == T){
    # plot the top ten terms for each topic in order
    top_terms %>% # take the top terms
      mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
      ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
      geom_col(show.legend = FALSE) + # as a bar plot
      facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
      labs(x = NULL, y = "Beta") + # no x label, change y label 
      coord_flip() # turn bars sideways
  }else{ 
    # if the user does not request a plot
    # return a list of sorted terms instead
    return(top_terms)
  }
}

# create a document term matrix to clean
Corpus_2004 <- Corpus(VectorSource(data_2004$headline_text)) 
DTM_2004 <- DocumentTermMatrix(Corpus_2004)

# convert the document term matrix to a tidytext corpus
DTM_tidy_2004 <- tidy(DTM_2004)

# I'm going to add my own custom stop words that I don't think will be
# very informative
custom_stop_words_2004 <- tibble(word = c("australia", "australian", 
                                          "queensland", "nsw", "victoria"))

# remove stopwords
DTM_tidy_cleaned_2004 <- DTM_tidy_2004 %>% # take our tidy dtm and...
  anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
  anti_join(custom_stop_words_2004, by = c("term" = "word")) # remove my custom stopwords

# reconstruct cleaned documents (so that each word shows up the correct number of times)
# stem the words (e.g. convert each word to its stem, where applicable)
cleaned_documents_2004 <- DTM_tidy_cleaned_2004 %>%
  group_by(document) %>% 
  mutate(terms = toString(rep(wordStem(term), count))) %>%
  select(document, terms) %>%
  unique()
cleaned_documents_2004
```

### Number of topics

Let's analyze how subjects change when their number changes. Since the texts analyzed are relatively short, we narrow the number of words describing a topic to 3.

#### 4 topics

Four themes seem to describe the news well. We describe the topics as follows:

* 1. **government plans**
* 2. **Iraq war plans**
* 3. **government and police**
* 4. **current politics**

However, 4 topics seems too small a number for a year's worth of news.  Let's see how it looks for 8 topics.

```{r top_terms_4_2004}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2004$terms, number_of_topics = 4)
```

#### 8 topics

Eight topics describe the news better than four topics. We describe the topics as follows:

* 1. **murder**
* 2. **police casual intervention**
* 3. **police crime intervention**
* 4. **police**
* 5. **fires**
* 6. **police with government**
* 7. **politics**
* 8. **water**

The division seems better than for 4 topics, but sometimes it is difficult to see what the difference is between the different topics containing the word *police*.
Let's still check how the 16 topics will look like.

```{r top_terms_8_2004}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2004$terms, number_of_topics = 8)
```

#### 16 topics

With sixteen topics, it is already very difficult to pinpoint what each topic is about.

However, there are topics that we have not observed before such as:

* 11. **drug lawsuit**
* 15. **layoffs**

```{r top_terms_16_2004}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2004$terms, number_of_topics = 16)
```

### Conclusions

In this chapter, we conducted topic modeling for 2004 data. Identifying the correct number of topics proved to be a difficult task. The main themes were:

* **police**
* **politics**
* **crimes**

A factor that negatively affected the evaluation of topics could be the length of the texts analyzed (no longer than 10 words, mostly 6 words).

Nevertheless, the indicated topics overlap with those identified through clustering. 

# Year of 2020

We conduct an analogous analysis for 2020. We repeat all the steps similarly as for 2004. In the process, we run into the problem of the dominance of the coronavirus topic over other news topics. We try to remedy this problem by removing the word from the text.

## Clustering

As for 2004, here we also perform k-means clustering. We use the Elbow method and Silhouette to indicate the correct number of clusters. We compare this number with the arbitrarily indicated number of clusters. We present the results using wordclouds.

### Data cleaning

The same data preparation steps as for 2004:

* removing punctuation
* remove numbers
* transforming to lowercase
* remove stopwords
* removing custom words
* reducing sparcity
* calculating distance matrix (euclidian)

```{r load_cleaning_2020}
docs_2020 <- VCorpus(VectorSource(data_2020$headline_text))

# Preliminary cleaning, Cleaning text and Stopword removal ----
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))

docs_2020 <- tm_map(docs_2020, removePunctuation)
docs_2020 <- tm_map(docs_2020, removeNumbers)
docs_2020 <- tm_map(docs_2020, content_transformer(tolower))
docs_2020 <- tm_map(docs_2020, removeWords, stopwords("english"))
docs_2020 <- tm_map(docs_2020, removeWords, c("australia", "australian", "nsw", "queensland", "victoria")) # since the source data is from Australia, remove this word from the dataset
docs_2020 <- tm_map(docs_2020, stripWhitespace)

# Create document-term matrix
tdm_2020 <- TermDocumentMatrix(docs_2020)

# Reduce sparcity
tdms_2020 <- removeSparseTerms(tdm_2020, sparse = 0.99)

# Calculate distance matrix
best_n_matrix_2020 <- as.matrix(dist(tdms_2020, method="euclidian"))
```

### Find optimal number of clusters

Just as for 2004 we propose two approaches. One based solely on statistical methods. The other based on observations resulting from abitrary selection of number of clusters. Then we compare which method gives better results.

#### Elbow method

More on the Elbow method above (for 2004).

```{r elbow_2020}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(subtitle = "Elbow method")
```

In contrast to the Elbow method for 2004, we see a much clearer indication for the 2 clusters (a significant break in the curve). Elbow method clearly indicates that the correct number of clusters is 2.

#### Silhouette method

More on the Silhouette method above (for 2004).

```{r silhouette_2020}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Similarly for the Silhouette method. Much clearer indication of 2 clusters than previously for 2004.

### Visualize clusters

Let's see how the visualization of clustering results for 2 clusters looks like.

#### Clustplot

Visualization of clusters on a 2D chart.

```{r clust_2_2020}
kfit_2020 <- kmeans(dist(tdms_2020, method="euclidian"), 2)

clusplot(as.matrix(dist(tdms_2020, method="euclidian")), kfit_2020$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

We see a very clear division into two groups. Basically, the division is based on the division of COVID - everything else. We will later evaluate whether this division is correct.

#### Hierarchical clustering

```{r hier_2_2020}
d_2020 <- dist(tdms_2020, method="euclidian")

hc_2020 <- hclust(d_2020, "ward.D")

# Kmeans
clustering_2020 <- kfit_2020$cluster

plot(hc_2020, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

The hierarchical tree visualization shows this division even more clearly.

```{r summ_2020, echo=FALSE, include=FALSE}
p_words_2020 <- colSums(t(as.matrix(tdms_2020))) / sum(t(as.matrix(tdms_2020)))

cluster_words_2020 <- lapply(unique(clustering_2020), function(x){
  rows_2020 <- tdms_2020[ clustering_2020 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2020 <- rows_2020[ , colSums(t(as.matrix(rows_2020))) > 0 ]
  
  colSums(t(as.matrix(rows_2020))) / sum(t(as.matrix(rows_2020))) - p_words_2020[ colnames(t(as.matrix(rows_2020))) ]
})

# create a summary table of the top 5 words defining each cluster
cluster_summary_2020 <- data.frame(cluster = unique(clustering_2020),
                                   size = as.numeric(table(clustering_2020)),
                                   top_words = sapply(cluster_words_2020, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2020
```

#### Word Cloud for Cluster No. 1

```{r wordcloud_1_2_2020}
wordcloud(words = names(cluster_words_2020[[ 1 ]]), 
          freq = cluster_words_2020[[ 1 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The first cluster included words that ambiguously point to a single theme. The words *new* and *cases* point to a COVID-related topic. The words *trump* and *says* point to some speech by US President Donald Trump.

#### Word Cloud for Cluster No. 2

```{r wordcloud_2_2_2020}
wordcloud(words = names(cluster_words_2020[[ 2 ]]), 
          freq = cluster_words_2020[[ 2 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

In the second cluster, unsurprisingly, only *coronavirus* was found.

### Improvement attempt

The year 2020 was a very special year. In fact, it was dominated entirely by news about coronavirus. For this reason, text analysis is hampered. Topics around coronavirus will make up the vast majority of news topics.

In order to improve the quality of clustering (later also topic modeling), we decide to completely remove the word *coronavirus* from our analysis.

```{r load_cleaning_no_covid}
docs_2020 <- tm_map(docs_2020, removeWords, c("coronavirus")) # remove 'coronavirus'

# Create document-term matrix
tdm_2020 <- TermDocumentMatrix(docs_2020)

# Reduce sparcity
tdms_2020 <- removeSparseTerms(tdm_2020, sparse = 0.99)

# Calculate distance matrix
best_n_matrix_2020 <- as.matrix(dist(tdms_2020, method="euclidian"))
```

#### Find optimal number of clusters

Let's check what the optimal number of clusters is now suggested by Elbow method and Silhouette method after removing the word *coronavirus*.

##### Elbow method

```{r elbow_2020_no_covid}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(subtitle = "Elbow method")
```

Elbow method no longer indicates such a strong division into 2 clusters. Nevertheless, the curve break for the 2 clusters is still visible. 

##### Silhouette method

```{r silhouette_2020_no_covid}
fviz_nbclust(best_n_matrix_2020, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Silhouette method also like Elbow method no longer so clearly indicates 2 clusters. However, two clusters should still be selected.

##### Visualize clusters

Let's visualize what the division into two clusters looks like.

###### Clustplot

```{r clust_2020_no_covid}
kfit_2020 <- kmeans(dist(tdms_2020, method="euclidian"), 2)

clusplot(as.matrix(dist(tdms_2020, method="euclidian")), kfit_2020$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

We recognize that the division into two clusters is not correct. Among other things, it can be observed that the words *donald* and *trump* fell into two separate clusters. Later we try to increase the number of clusters and compare the results.

###### Hierarchical clustering

```{r hier_2020_no_covid}
d_2020 <- dist(tdms_2020, method="euclidian")

hc_2020 <- hclust(d_2020, "ward.D")

# Kmeans
clustering_2020 <- kfit_2020$cluster

plot(hc_2020, main = "Hierarchical clustering",
     ylab = "", xlab = "", yaxt = "n")
```

The incorrect division of clusters is confirmed by hierarchical tree analysis. The words *donald* and *trump* are close to each other and should not go into separate clusters.

```{r summ_2020_no_covid, echo=FALSE, include=FALSE}
p_words_2020 <- colSums(t(as.matrix(tdms_2020))) / sum(t(as.matrix(tdms_2020)))

cluster_words_2020 <- lapply(unique(clustering_2020), function(x){
  rows_2020 <- tdms_2020[ clustering_2020 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2020 <- rows_2020[ , colSums(t(as.matrix(rows_2020))) > 0 ]
  
  colSums(t(as.matrix(rows_2020))) / sum(t(as.matrix(rows_2020))) - p_words_2020[ colnames(t(as.matrix(rows_2020))) ]
})



# create a summary table of the top 5 words defining each cluster
cluster_summary_2020 <- data.frame(cluster = unique(clustering_2020),
                                   size = as.numeric(table(clustering_2020)),
                                   top_words = sapply(cluster_words_2020, function(d){
                                     paste(
                                       names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                       collapse = ", ")
                                   }),
                                   stringsAsFactors = FALSE)

cluster_summary_2020
```

##### Arbitrary choice

So we choose the number of clusters arbitrarily. Let's see the results for 4 clusters.

###### Clustplot

```{r clust_2020_4_no_covid}
kfit_2020 <- kmeans(dist(tdms_2020, method="euclidian"), 4)

clustering_2020 <- kfit_2020$cluster

cluster_words_2020 <- lapply(unique(clustering_2020), function(x){
  rows_2020 <- tdms_2020[ clustering_2020 == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows_2020 <- rows_2020[ , colSums(t(as.matrix(rows_2020))) > 0 ]
  
  colSums(t(as.matrix(rows_2020))) / sum(t(as.matrix(rows_2020))) - p_words_2020[ colnames(t(as.matrix(rows_2020))) ]
})

clusplot(as.matrix(dist(tdms_2020, method="euclidian")), kfit_2020$cluster, color=T, shade=T, labels=2, lines=0, 
         main = "2D Representation of Clusters")
```

The results look much more promising. We have a breakdown of Donald Trump and three other clusters related to different per-pandemic topics.

###### Word Cloud for Cluster No. 1

```{r wordcloud_1_4_2020_no_covid}
wordcloud(words = names(cluster_words_2020[[ 1 ]]), 
          freq = cluster_words_2020[[ 1 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The first cluster can be described as describing **court rulings on new panemic regulations** and their links to health.

###### Word Cloud for Cluster No. 2

```{r wordcloud_2_4_2020_no_covid}
wordcloud(words = names(cluster_words_2020[[ 2 ]]), 
          freq = cluster_words_2020[[ 2 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

We describe the second cluster as **new restrictions introduced by the government** due to the number of cases.

###### Word Cloud for Cluster No. 3

```{r wordcloud_3_4_2020_no_covid}
wordcloud(words = names(cluster_words_2020[[ 3 ]]), 
          freq = cluster_words_2020[[ 3 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The third cluster is **Donald Trump's speeches**.

###### Word Cloud for Cluster No. 4

```{r wordcloud_4_4_2020_no_covid}
wordcloud(words = names(cluster_words_2020[[ 4 ]]), 
          freq = cluster_words_2020[[ 4 ]], max.words=5, rot.per=0.2, colors = brewer.pal(6, "Dark2"))
```

The fourth cluster is the **new responsibilities of the police**.

### Conclusions

We performed clusterization on data from 2020. Because of the very prevalent coronavirus theme that year, the quality of clustering was not satisfactory. To improve, we removed the word *coronavirus* from the analysis. As a result, we noticed an improvement in the results.

We fixed the number of clusters at 4. We described them as follows:

* 1. **court rulings on new panemic regulations**
* 2. **new restrictions introduced by the government**
* 3. **Donald Trump's speeches**
* 4. **new responsibilities of the police**

## Topic Modeling

With the topic of clustering, we showed that removing the word *coronavirus* improves the analysis results. We perform Topic Modeling immediately on the text with the word *coronavirus* removed.

### Data preparation

Data preparation analogous to Topic Modeling done for 2004. In addition, at the very beginning we remove the word *coronavirus*.

```{r topic_modeling_preli_2020}
# create a document term matrix to clean
Corpus_2020 <- Corpus(VectorSource(data_2020$headline_text)) 
DTM_2020 <- DocumentTermMatrix(Corpus_2020)

# convert the document term matrix to a tidytext corpus
DTM_tidy_2020 <- tidy(DTM_2020)

# I'm going to add my own custom stop words that I don't think will be
# very informative
custom_stop_words_2020 <- tibble(word = c("australia", "australian", 
                                          "melbourne", "queensland", "nsw", "victoria",
                                          "covid", "coronavirus"))

# remove stopwords
DTM_tidy_cleaned_2020 <- DTM_tidy_2020 %>% # take our tidy dtm and...
  anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
  anti_join(custom_stop_words_2020, by = c("term" = "word")) # remove my custom stopwords

# reconstruct cleaned documents (so that each word shows up the correct number of times)
# stem the words (e.g. convert each word to its stem, where applicable)
cleaned_documents_2020 <- DTM_tidy_cleaned_2020 %>%
  group_by(document) %>% 
  mutate(terms = toString(rep(wordStem(term), count))) %>%
  select(document, terms) %>%
  unique()
cleaned_documents_2020
```

### Number of topics

As before, we look at how subjects change due to changes in their numbers.

#### 4 topics

Let's check out the breakdown of the 4 topics.

```{r top_terms_4_2020}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2020$terms, number_of_topics = 4)
```

At first glance, the division into 4 topics seems correct, however, perhaps it is too simple. We propose the following description of the topics:

* 1. **current information on government policy**
* 2. **reports on deaths**
* 3. **news from the state of Victoria**
* 4. **new bushfires**

#### 8 topics

Let's check out the breakdown of the 8 topics.

```{r top_terms_8_2020}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2020$terms, number_of_topics = 8)
```

The naming of the topics resulting from the division into 8 is not clear. We distinguish such topics as:

* 1. **current information on government policy**
* 2. **reports on deaths in Sydney**
* 4. **fires crossing the (state) border**
* 5. **impact of fires on pandemic**
* 7. **statements made by Donald Trump about China**

The remaining topics are not identified.

#### 16 topics

Let's check out the breakdown of the 16 topics.

```{r top_terms_16_2020}
# now let's look at the new most informative terms
top_terms_by_topic_LDA(cleaned_documents_2020$terms, number_of_topics = 16)
```

The naming of the topics created by the 16 division is even less clear-cut than that of the 8 division. Nevertheless, we distinguish such topics as:

* 1. **current information on election results**
* 2. **reports on deaths in Sydney**
* 4. **fires crossing the (state) border**
* 6. **health in the context of the US election**
* 8. **fire danger to buildings**
* 10. **COVID policy in China**
* 13. **fear of death**

The remaining topics are not identified.

### Conclusions

In this section, we conducted Topic Modeling for 2020. Identifying topics proved to be a difficult challenge. We believe that (as with the analysis for 2004) this was influenced by the length of the texts analyzed (no longer than 10 words, mostly 6 words)

We identified the following topics:

* **reports on COVID**
* **elections in the United States**
* **bushfires**

The topics overlap with those identified by clustering.

# Final Conclusions

In this project, we used news headline data for 2004 and 2020 to perform text clustering and topic modeling. We used Elbow and Silhouette methods to identify the optimal number of clusters and compared the results with an arbitrary number of clusters. We showed the results in the graphs.

For both the 2004 and 2020 analysis, clustering with an arbitrarily chosen number of clusters turned out to be better. The topics identified by Text Clustering and Topic Modeling were similar and consistent.

For 2004, we identified the following topics:

* **judiciary**
* **Iraq war politics**
* **politics**
* **elections**
* **general topics related to police intervention**

For 2020, we identified the following themes:

* **court rulings on new panemic regulations**
* **new restrictions introduced by the government**
* **new responsibilities of the police**
* **reports on COVID**
* **elections in the United States**
* **bushfires**

We also note that it was not always possible to identify a topic. We point to problematic input in the form of very short texts (maximum 10 words) as the reason. For this reason, we did not have enough words per text to be able to uniquely identify some topics.
